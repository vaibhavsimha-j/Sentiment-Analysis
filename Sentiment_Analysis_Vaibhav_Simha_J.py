# -*- coding: utf-8 -*-
"""Sentiment Analysis - Vaibhav Simha J.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Pm8Wku635AWsl4KBh91Q4RIQPi17J5Ua

US Airline Tweets Sentiment Analysis |
Author: Vaibhav Simha J |
Created: MAY 2025 |
Description: LSTM-based NLP project to classify sentiments using TensorFlow and real-world tweet data.
"""

# Importing required Libraries
import pandas as pd
import matplotlib.pyplot as plt

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM,Dense, Dropout, SpatialDropout1D
from tensorflow.keras.layers import Embedding

df = pd.read_csv("/content/drive/MyDrive/Sentiment Analysis/Tweets.csv")

# Mounting Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Displaying the first 5 rows of the original dataset to preview its structure and contents
df.head()

#Displaying all the Columns present in the dataset
df.columns

# Extracting only the tweet text and sentiment columns from the dataset for analysis
# Displaying the shape and first 5 rows of the new DataFrame
tweet_df = df[['text','airline_sentiment']]
print(tweet_df.shape)
tweet_df.head(5)

# Removing tweets labeled as 'neutral' to focus only on positive and negative sentiments
# Displaying the new shape and first 5 rows after filtering
tweet_df = tweet_df[tweet_df['airline_sentiment'] != 'neutral']
print(tweet_df.shape)
tweet_df.head(5)

# Counting the number of positive and negative sentiment labels in the dataset
tweet_df["airline_sentiment"].value_counts()

# Converting sentiment labels (positive/negative) into numeric form (e.g., 0 and 1) for model training
sentiment_label = tweet_df.airline_sentiment.factorize()
sentiment_label

# Extracting tweet texts and initializing a tokenizer to convert words into numeric tokens (limited to top 5000 words)
tweet = tweet_df.text.values
tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(tweet)

# Calculating the vocabulary size (total unique words + 1 for padding/indexing)
vocab_size = len(tokenizer.word_index) + 1

# Converting tweets to sequences of integers and padding them to ensure equal input length (maxlen = 200)
encoded_docs = tokenizer.texts_to_sequences(tweet)
padded_sequence = pad_sequences(encoded_docs, maxlen=200)

# Printing the word-to-index dictionary showing how each word was tokenized
print(tokenizer.word_index)

# Displaying the original tweet and its corresponding tokenized (numerical) form
print(tweet[0])
print(encoded_docs[0])

# Displaying the padded version of the first tweet (all sequences are now of equal length)
print(padded_sequence[0])

# Setting the dimensionality of the word embedding vectors
embedding_vector_length = 32

# Initializing a sequential model
model = Sequential()

# Adding an embedding layer to convert word indices into dense vectors of fixed size
model.add(Embedding(vocab_size, embedding_vector_length, input_length=200) )

# Adding a dropout layer to reduce overfitting by randomly dropping 25% of the embedding layerâ€™s output
model.add(SpatialDropout1D(0.25))

# Adding an LSTM layer with 50 units and dropout for both input and recurrent connections
model.add(LSTM(50, dropout=0.5, recurrent_dropout=0.5))

# Additional dropout layer after LSTM to further prevent overfitting
model.add(Dropout(0.2))

# Output layer with sigmoid activation for binary classification (positive vs negative sentiment)
model.add(Dense(1, activation='sigmoid'))

# Compiling the model with binary crossentropy loss and Adam optimizer
model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])

# Printing the model summary to view the architecture
print(model.summary())

# Training the model on the padded tweet sequences and sentiment labels
# Using 20% of the data for validation, running for 5 epochs with a batch size of 32
history = model.fit(padded_sequence,sentiment_label[0],validation_split=0.2, epochs=5, batch_size=32)

# Plotting training and validation accuracy over epochs
plt.plot(history.history['accuracy'], label='acc')
plt.plot(history.history['val_accuracy'], label='val_acc')
plt.legend()
plt.show()
plt.savefig("Accuracy plot.jpg")

# Plotting training and validation loss over epochs
plt.plot(history.history['loss'], label='loss')
plt.plot(history.history['val_loss'], label='val_loss')
plt.legend()
plt.show()
plt.savefig("Loss plot.jpg")

# Function to predict sentiment for a given input text
def predict_sentiment(text):
    tw = tokenizer.texts_to_sequences([text])
    tw = pad_sequences(tw,maxlen=200)
    prediction = int(model.predict(tw).round().item())
    print("Predicted label: ", sentiment_label[1][prediction])

# Testing / Test Case
test_sentence1 = "I enjoyed my journey on this flight."
predict_sentiment(test_sentence1)

test_sentence2 = "This is the worst flight experience of my life!"
predict_sentiment(test_sentence2)